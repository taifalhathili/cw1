{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\c2034122\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\c2034122\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\c2034122\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Library for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# os library is used here for file path locatain\n",
    "import os\n",
    "\n",
    "# numpy is used for numerical operation\n",
    "import numpy as np\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In this piece of code, we extract the data from text file and put it in columns\n",
    "\n",
    "# \"bbc\" is the folder of the dataset\n",
    "directory = 'bbc'\n",
    "\n",
    "# we created a dataframe with two columns \"news\" that is text related to business, tech etc and a category for text\n",
    "data_frame = pd.DataFrame(columns=['news', 'category'])\n",
    "\n",
    "id_count = 0\n",
    "\n",
    "# In the below code the for loop will go through the text files and get the text from each file and put it in dataframe column.\n",
    "# the folders name is tech, business etc. and it is pushed to \"news\" columns\n",
    "for subdir, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if not file.startswith(\"READ\"):\n",
    "            category = subdir.replace('bbc\\\\','')\n",
    "            \n",
    "            with open(os.path.join(subdir, file)) as f:\n",
    "                data = \" \".join(line.rstrip() for line in f)\n",
    "                data_frame = data_frame.append(pd.DataFrame([[data, category]], columns=['news', 'category']), ignore_index = True)\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_frame.to_csv('Prepared_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only to upload csv file to colab\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('Prepared_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate Category names with numerical index and save it in new column category_id\n",
    "data_frame['category_id'] = data_frame['category'].factorize()[0]\n",
    "\n",
    "# map the categories_name to number\n",
    "category_id_df = data_frame[['category', 'category_id']].drop_duplicates().sort_values('category_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit  Quarterly p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech  The dollar h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim  The owners ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits  British Air...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq  Shares in U...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 news  category_id\n",
       "ID                                                                \n",
       "0   Ad sales boost Time Warner profit  Quarterly p...            0\n",
       "1   Dollar gains on Greenspan speech  The dollar h...            0\n",
       "2   Yukos unit buyer faces loan claim  The owners ...            0\n",
       "3   High fuel prices hit BA's profits  British Air...            0\n",
       "4   Pernod takeover talk lifts Domecq  Shares in U...            0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert ID column\n",
    "data_frame['ID'] = range(0, len(data_frame))\n",
    "\n",
    "# set ID as an index\n",
    "data_frame = data_frame.set_index('ID')\n",
    "\n",
    "# Dropping the category column. here we can also use the drop function\n",
    "data_frame = data_frame[['news', 'category_id']]\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataframe for machine learning model\n",
    "from sklearn.utils import shuffle\n",
    "data_frame = shuffle(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>Franz man seeks government help  Franz Ferdina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>US show sued for rat-eating stunt  A US TV net...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>Police urge pub closure power  New powers are ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Europe asks Asia for euro help  European leade...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>US regulator to rule on pain drug  US food and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   news  category_id\n",
       "ID                                                                  \n",
       "640   Franz man seeks government help  Franz Ferdina...            1\n",
       "718   US show sued for rat-eating stunt  A US TV net...            1\n",
       "1060  Police urge pub closure power  New powers are ...            2\n",
       "495   Europe asks Asia for euro help  European leade...            0\n",
       "190   US regulator to rule on pain drug  US food and...            0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will remove the garbage data that will not be helpful to our model\n",
    "def data_preprocessing(news):\n",
    "    # Replace new line with null if any exist also remove the carriage return\n",
    "    news = news.lower().replace('\\n', ' ').replace('\\r', '').strip()\n",
    "    # to remove multiple spaces we will be using a regex expression. it will replace\n",
    "    # multiple spaces with single space\n",
    "    news = re.sub(' +', ' ', news)\n",
    "    # to get alphabets\n",
    "    news = re.sub(r'[^\\w\\s]', '', news)\n",
    "    \n",
    "    # we will be removing stopwords like and, or etc.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # list of words include in news article\n",
    "    word_tokens = word_tokenize(news)\n",
    "    # this will remove all unnecessary words specified in nltk stopwords file\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    news = ' '.join(filtered_sentence)\n",
    "    return news\n",
    "\n",
    "data_frame['news'] = data_frame['news'].apply(data_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)\n",
    "\n",
    "data_frame['news_noun_adj'] = data_frame.news.apply(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['news'] = data_frame['news'] + data_frame['news_noun_adj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate word from a string.\n",
    "data_frame['news'] = data_frame.news.str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for category and their ID.\n",
    "# we need it later\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'category']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "features = tfidf.fit_transform(data_frame.news).toarray() \n",
    "\n",
    "labels = data_frame.category_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 11719)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>category_id</th>\n",
       "      <th>news_noun_adj</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>franz man seeks government help ferdinand fron...</td>\n",
       "      <td>1</td>\n",
       "      <td>franz man government help ferdinand frontman a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>us show sued rateating stunt tv network 25m 창1...</td>\n",
       "      <td>1</td>\n",
       "      <td>sued stunt network 창13m viewer contestants dea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>police urge pub closure power new powers neede...</td>\n",
       "      <td>2</td>\n",
       "      <td>police pub closure power new powers disorderly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>europe asks asia euro help european leaders sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>europe asia euro help european leaders asian s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>us regulator rule pain drug food regulators de...</td>\n",
       "      <td>0</td>\n",
       "      <td>rule drug food drug regulators recommend sale ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   news  category_id  \\\n",
       "ID                                                                     \n",
       "640   franz man seeks government help ferdinand fron...            1   \n",
       "718   us show sued rateating stunt tv network 25m 창1...            1   \n",
       "1060  police urge pub closure power new powers neede...            2   \n",
       "495   europe asks asia euro help european leaders sa...            0   \n",
       "190   us regulator rule pain drug food regulators de...            0   \n",
       "\n",
       "                                          news_noun_adj  \n",
       "ID                                                       \n",
       "640   franz man government help ferdinand frontman a...  \n",
       "718   sued stunt network 창13m viewer contestants dea...  \n",
       "1060  police pub closure power new powers disorderly...  \n",
       "495   europe asia euro help european leaders asian s...  \n",
       "190   rule drug food drug regulators recommend sale ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'business':\n",
      "  . unigrams: economic, prices, investors, stock, economy, analysts, oil, bank, growth, shares\n",
      "  . bigrams: analysts said, stock exchange, securities exchange, exchange commission, told reuters, economic growth, news agency, chief executive, oil prices, stock market\n",
      "'entertainment':\n",
      "  . unigrams: stars, starring, album, award, comedy, awards, actress, singer, actor, film\n",
      "  . bigrams: golden globe, leonardo dicaprio, film festival, named best, imelda staunton, vera drake, dollar baby, million dollar, box office, los angeles\n",
      "'politics':\n",
      "  . unigrams: leader, tony, liberal, secretary, party, blair, tories, tory, election, labour\n",
      "  . bigrams: tory leader, radio 4s, liberal democrat, leader michael, lib dems, prime minister, liberal democrats, michael howard, general election, tony blair\n",
      "'sport':\n",
      "  . unigrams: rugby, victory, champion, win, game, season, injury, match, cup, coach\n",
      "  . bigrams: coach andy, sir alex, manchester united, rbs nations, australian open, world cup, world number, champions league, bbc sport, grand slam\n",
      "'tech':\n",
      "  . unigrams: devices, microsoft, web, pc, online, digital, computer, software, technology, users\n",
      "  . bigrams: playstation portable, hugely popular, media player, open source, sony psp, hard drive, mobile phones, bbc news, consumer electronics, news website\n"
     ]
    }
   ],
   "source": [
    "# Importing chi-square for feature selection\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# number of words in a perticular set\n",
    "Num_of_words = 10\n",
    "\n",
    "# Find correlation between words and categories\n",
    "for Category, category_id in sorted(category_to_id.items()):\n",
    "    # apply chi-square to all features in categories.\n",
    "    features_chi2 = chi2(features, labels == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    \n",
    "    # list of top unigrams with chi-square.\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    \n",
    "    # list of top bigrams with chi-square.\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    \n",
    "    # display the category\n",
    "    print(\"'{}':\".format(Category))\n",
    "    \n",
    "    # display top 10 unigrams\n",
    "    print(\"  . unigrams: {}\".format(', '.join(unigrams[-Num_of_words:])))\n",
    "    \n",
    "    # display top 10 bigrams\n",
    "    print(\"  . bigrams: {}\".format(', '.join(bigrams[-Num_of_words:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# we are using Decision tree classifier\n",
    "model_dec_tree = DecisionTreeClassifier(random_state=1)\n",
    "model_log_reg = LogisticRegression(random_state=1)\n",
    "model_naive = GaussianNB()\n",
    "model_svc = SVC(random_state=1)\n",
    "\n",
    "#Split Data \n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, data_frame.index, test_size=0.20, random_state=2)\n",
    "\n",
    "#Train model\n",
    "model_dec_tree.fit(X_train, y_train)\n",
    "model_log_reg.fit(X_train, y_train)\n",
    "model_naive.fit(X_train, y_train)\n",
    "model_svc.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred_dec_tree = model_dec_tree.predict(X_test)\n",
    "y_pred_log_reg = model_log_reg.predict(X_test)\n",
    "y_pred_naive = model_naive.predict(X_test)\n",
    "y_pred_svc = model_svc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision tree:  0.7955056179775281\n",
      "Accuracy of Logistic regression:  0.9707865168539326\n",
      "Accuracy of Naive bayes:  0.9235955056179775\n",
      "Accuracy of SVC:  0.9752808988764045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy of Decision tree: \", accuracy_score(y_test, y_pred_dec_tree))\n",
    "print(\"Accuracy of Logistic regression: \", accuracy_score(y_test, y_pred_log_reg))\n",
    "print(\"Accuracy of Naive bayes: \", accuracy_score(y_test, y_pred_naive))\n",
    "print(\"Accuracy of SVC: \", accuracy_score(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78        97\n",
      "           1       0.84      0.85      0.84        80\n",
      "           2       0.70      0.69      0.69        80\n",
      "           3       0.85      0.85      0.85       100\n",
      "           4       0.81      0.78      0.80        88\n",
      "\n",
      "    accuracy                           0.80       445\n",
      "   macro avg       0.79      0.79      0.79       445\n",
      "weighted avg       0.80      0.80      0.80       445\n",
      "\n",
      "Classification report for Logistic regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96        97\n",
      "           1       0.97      0.95      0.96        80\n",
      "           2       0.95      0.95      0.95        80\n",
      "           3       1.00      0.99      0.99       100\n",
      "           4       0.99      0.97      0.98        88\n",
      "\n",
      "    accuracy                           0.97       445\n",
      "   macro avg       0.97      0.97      0.97       445\n",
      "weighted avg       0.97      0.97      0.97       445\n",
      "\n",
      "Classification report for Naive bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91        97\n",
      "           1       0.99      0.84      0.91        80\n",
      "           2       0.86      0.95      0.90        80\n",
      "           3       1.00      0.95      0.97       100\n",
      "           4       0.87      0.98      0.92        88\n",
      "\n",
      "    accuracy                           0.92       445\n",
      "   macro avg       0.93      0.92      0.92       445\n",
      "weighted avg       0.93      0.92      0.92       445\n",
      "\n",
      "Classification report for SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96        97\n",
      "           1       0.97      0.97      0.97        80\n",
      "           2       0.96      0.96      0.96        80\n",
      "           3       1.00      0.99      0.99       100\n",
      "           4       1.00      0.97      0.98        88\n",
      "\n",
      "    accuracy                           0.98       445\n",
      "   macro avg       0.98      0.97      0.97       445\n",
      "weighted avg       0.98      0.98      0.98       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification report for Decision Tree')\n",
    "print(classification_report(y_test, y_pred_dec_tree))\n",
    "print('Classification report for Logistic regression')\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "print('Classification report for Naive bayes')\n",
    "print(classification_report(y_test, y_pred_naive))\n",
    "print('Classification report for SVC')\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input news:FA Cup: Watch all Leicester's goals from their journey to the semi-finals\n",
      "Decision tree Predicted as:  sport\n"
     ]
    }
   ],
   "source": [
    "# Decision tree: testing the model on user input data\n",
    "text_features = tfidf.transform([input('Input news:')])\n",
    "prediction = model_dec_tree.predict(text_features)\n",
    "\n",
    "print(\"Decision tree Predicted as: \", id_to_category[prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input news:Prince Philip funeral: William and Harry seen chatting after ceremony\n",
      "Logistic regression Predicted as:  entertainment\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression: testing the model on user input data\n",
    "text_features = tfidf.transform([input('Input news:')])\n",
    "prediction = model_log_reg.predict(text_features)\n",
    "\n",
    "print(\"Logistic regression Predicted as: \", id_to_category[prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
